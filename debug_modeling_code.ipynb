{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_MODEL = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/model_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_MODEL = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/model_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "INFINI_TOKEN_EMBEDDING = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/model.token_position_embeddings_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_TOKEN_EMBEDDING = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/model.token_position_embeddings_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "infini_model_acts = torch.load(INFINI_MODEL)\n",
    "ref_model_acts = torch.load(REF_MODEL)\n",
    "\n",
    "infini_token_embedding_acts = torch.load(INFINI_TOKEN_EMBEDDING)\n",
    "ref_token_embedding_acts = torch.load(REF_TOKEN_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0469, -0.0186, -0.0439,  ...,  0.0469, -0.0197, -0.0157],\n",
       "         [-0.0527,  0.0161,  0.0659,  ...,  0.0079, -0.0142,  0.0410],\n",
       "         [-0.0327, -0.0166, -0.0159,  ..., -0.0082, -0.0483,  0.0405],\n",
       "         ...,\n",
       "         [ 0.0488, -0.0177,  0.0162,  ..., -0.0177, -0.0109,  0.0183],\n",
       "         [ 0.0244,  0.0160, -0.0165,  ...,  0.0415, -0.0172, -0.0171],\n",
       "         [-0.0251, -0.0232,  0.0300,  ...,  0.0179,  0.0212, -0.0339]],\n",
       "\n",
       "        [[-0.0527,  0.0161,  0.0659,  ...,  0.0079, -0.0142,  0.0410],\n",
       "         [-0.0094,  0.0840,  0.0172,  ..., -0.0036, -0.0486, -0.0299],\n",
       "         [-0.0325, -0.0767,  0.0601,  ..., -0.0552, -0.0079,  0.0315],\n",
       "         ...,\n",
       "         [-0.0359, -0.0145,  0.0581,  ..., -0.0157, -0.0312,  0.0125],\n",
       "         [-0.0325, -0.0767,  0.0601,  ..., -0.0552, -0.0079,  0.0315],\n",
       "         [-0.0075, -0.0366,  0.0645,  ...,  0.0605, -0.0114,  0.0359]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_token_embedding_acts[\"input_embeds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 1024]), torch.Size([256, 2, 1024]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_token_embedding_acts[\"input_embeds\"].shape, ref_token_embedding_acts[\"input_embeds\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0469, -0.0186, -0.0439, -0.0566],\n",
       "         [-0.0527,  0.0161,  0.0659, -0.0195]],\n",
       "\n",
       "        [[-0.0527,  0.0161,  0.0659, -0.0195],\n",
       "         [-0.0094,  0.0840,  0.0172, -0.0193]],\n",
       "\n",
       "        [[-0.0327, -0.0166, -0.0159, -0.0060],\n",
       "         [-0.0325, -0.0767,  0.0601, -0.0275]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0488, -0.0177,  0.0162,  0.0131],\n",
       "         [-0.0359, -0.0145,  0.0581, -0.0449]],\n",
       "\n",
       "        [[ 0.0244,  0.0160, -0.0165,  0.0216],\n",
       "         [-0.0325, -0.0767,  0.0601, -0.0275]],\n",
       "\n",
       "        [[-0.0251, -0.0232,  0.0300,  0.0028],\n",
       "         [-0.0075, -0.0366,  0.0645,  0.0540]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_token_embedding_acts[\"input_embeds\"].transpose(0, 1)[:, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0469, -0.0186, -0.0439, -0.0566],\n",
       "         [-0.0527,  0.0161,  0.0659, -0.0195]],\n",
       "\n",
       "        [[-0.0527,  0.0161,  0.0659, -0.0195],\n",
       "         [-0.0094,  0.0840,  0.0172, -0.0193]],\n",
       "\n",
       "        [[-0.0327, -0.0166, -0.0159, -0.0060],\n",
       "         [-0.0325, -0.0767,  0.0601, -0.0275]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0488, -0.0177,  0.0162,  0.0131],\n",
       "         [-0.0359, -0.0145,  0.0581, -0.0449]],\n",
       "\n",
       "        [[ 0.0244,  0.0160, -0.0165,  0.0216],\n",
       "         [-0.0325, -0.0767,  0.0601, -0.0275]],\n",
       "\n",
       "        [[-0.0251, -0.0232,  0.0300,  0.0028],\n",
       "         [-0.0075, -0.0366,  0.0645,  0.0540]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_token_embedding_acts[\"input_embeds\"][:, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(infini_token_embedding_acts[\"input_embeds\"].transpose(0, 1), ref_token_embedding_acts[\"input_embeds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0156,  0.0437,  0.0312, -0.0518],\n",
       "         [ 0.0260,  0.0148, -0.0427,  0.0388],\n",
       "         [ 0.0160,  0.0601,  0.0188, -0.0165],\n",
       "         [-0.0549, -0.0052, -0.0292,  0.0164],\n",
       "         [ 0.0240, -0.0006,  0.0042,  0.0010]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_token_embedding_acts[\"input_embeds\"][:, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0156,  0.0437,  0.0312, -0.0518],\n",
       "         [ 0.0260,  0.0148, -0.0427,  0.0388],\n",
       "         [ 0.0160,  0.0601,  0.0188, -0.0165],\n",
       "         [-0.0549, -0.0052, -0.0292,  0.0164],\n",
       "         [ 0.0240, -0.0006,  0.0042,  0.0010]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_token_embedding_acts[\"input_embeds\"][:, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_BLOCK_1_INPUT_LN = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.input_layernorm_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_BLOCK_1_INPUT_LN = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.input_layernorm_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_block_1_input_ln = torch.load(INFINI_BLOCK_1_INPUT_LN)\n",
    "ref_block_1_input_ln = torch.load(REF_BLOCK_1_INPUT_LN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4727,  1.3203,  0.9453, -1.5625],\n",
       "          [ 0.8320,  0.4746, -1.3672,  1.2422],\n",
       "          [ 0.5352,  2.0156,  0.6289, -0.5547],\n",
       "          [-1.7188, -0.1641, -0.9141,  0.5117],\n",
       "          [ 0.7539, -0.0176,  0.1328,  0.0317]]], device='cuda:0',\n",
       "        dtype=torch.bfloat16),\n",
       " tensor([[[-0.4727,  1.3203,  0.9453, -1.5625]],\n",
       " \n",
       "         [[ 0.8320,  0.4746, -1.3672,  1.2422]],\n",
       " \n",
       "         [[ 0.5352,  2.0156,  0.6289, -0.5547]],\n",
       " \n",
       "         [[-1.7188, -0.1641, -0.9141,  0.5117]],\n",
       " \n",
       "         [[ 0.7539, -0.0176,  0.1328,  0.0317]]], device='cuda:0',\n",
       "        dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_block_1_input_ln[:, :, :4], ref_block_1_input_ln[:, :, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "INFINI_BLOCK_0_QKV = \"/fsx/phuc/projects/nanotron/debug/nn_states_with_bs_2/acts/model.decoder.0.pp_block.attn.qkv_proj_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "# INFINI_AFTER_FIX_BLOCK_0_QKV = \"/fsx/phuc/projects/nanotron/debug/nn_states_after_fix/acts/model.decoder.0.pp_block.attn.qkv_proj_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "REF_BLOCK_0_QKV = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states_with_bs_2/acts/model.decoder.0.pp_block.attn.qkv_proj_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_block_0_qkv = torch.load(INFINI_BLOCK_0_QKV)\n",
    "# infini_after_fix_block_0_qkv = torch.load(INFINI_AFTER_FIX_BLOCK_0_QKV)\n",
    "ref_block_0_qkv = torch.load(REF_BLOCK_0_QKV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 1536]), torch.Size([256, 2, 1536]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_block_0_qkv.shape, ref_block_0_qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(infini_block_0_qkv.transpose(0, 1), ref_block_0_qkv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKV embeddings before pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "INFINI_Q_BEFORE_POS = \"/fsx/phuc/projects/nanotron/debug/nn_states_with_bs_2/acts/query_states_before_pos_tp_rank_0.pt\"\n",
    "INFINI_Q_BEFORE_POS_AFTER_TRANSPOSE_QKV = \"/fsx/phuc/projects/nanotron/debug/nn_states_with_bs_2_and_transpose_qkv/acts/query_states_before_pos_tp_rank_0.pt\"\n",
    "\n",
    "REF_Q_BEFORE_POS = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states_with_bs_2/acts/query_states_before_pos_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_q_before_pos = torch.load(INFINI_Q_BEFORE_POS)\n",
    "infini_q_before_pos_and_after_transpose_qkv = torch.load(INFINI_Q_BEFORE_POS_AFTER_TRANSPOSE_QKV)\n",
    "ref_q_before_pos = torch.load(REF_Q_BEFORE_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 4, 128]),\n",
       " torch.Size([2, 256, 4, 128]),\n",
       " torch.Size([2, 256, 4, 128]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_q_before_pos.shape, infini_q_before_pos_and_after_transpose_qkv.shape, ref_q_before_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1719,  0.1797],\n",
       "          [ 0.9023, -0.0422],\n",
       "          [-1.3672, -1.0312],\n",
       "          [-0.3594, -0.8594]],\n",
       "\n",
       "         [[-0.5703, -2.5469],\n",
       "          [ 0.8164,  0.8906],\n",
       "          [ 0.1514, -1.6484],\n",
       "          [-0.9492,  0.6250]]],\n",
       "\n",
       "\n",
       "        [[[-0.6367, -0.1973],\n",
       "          [ 0.9062, -0.5859],\n",
       "          [-0.0515, -1.3359],\n",
       "          [-1.5234,  1.4375]],\n",
       "\n",
       "         [[-0.5469, -0.5742],\n",
       "          [-0.3770, -0.1777],\n",
       "          [-0.1553, -0.8945],\n",
       "          [ 0.1445, -0.2490]]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_q_before_pos[:, :2, :, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1719,  0.1797],\n",
       "          [ 0.9023, -0.0422],\n",
       "          [-1.3672, -1.0312],\n",
       "          [-0.3594, -0.8594]],\n",
       "\n",
       "         [[ 1.3594, -0.5586],\n",
       "          [-0.0435, -0.2393],\n",
       "          [ 0.1060, -1.3516],\n",
       "          [ 0.5703,  0.9883]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4922, -0.4570],\n",
       "          [ 0.3145, -0.5000],\n",
       "          [-0.1885, -0.9414],\n",
       "          [ 1.1172,  0.5938]],\n",
       "\n",
       "         [[-2.2969,  0.5039],\n",
       "          [ 0.0071,  0.5664],\n",
       "          [ 0.3242,  0.5742],\n",
       "          [ 1.7422, -0.6758]]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_q_before_pos[:, :2, :, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1719,  0.1797],\n",
       "          [ 0.9023, -0.0422],\n",
       "          [-1.3672, -1.0312],\n",
       "          [-0.3594, -0.8594]],\n",
       "\n",
       "         [[ 1.3594, -0.5586],\n",
       "          [-0.0435, -0.2393],\n",
       "          [ 0.1060, -1.3516],\n",
       "          [ 0.5703,  0.9883]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4922, -0.4570],\n",
       "          [ 0.3145, -0.5000],\n",
       "          [-0.1885, -0.9414],\n",
       "          [ 1.1172,  0.5938]],\n",
       "\n",
       "         [[-2.2969,  0.5039],\n",
       "          [ 0.0071,  0.5664],\n",
       "          [ 0.3242,  0.5742],\n",
       "          [ 1.7422, -0.6758]]]], device='cuda:0', dtype=torch.bfloat16,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_q_before_pos_and_after_transpose_qkv[:, :2, :, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(infini_q_before_pos_and_after_transpose_qkv, ref_q_before_pos, atol=0.05, rtol=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QKV embeddings after pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "INFINI_Q_AFTER_POS = \"/fsx/phuc/projects/nanotron/debug/nn_states_with_bs_2/acts/query_states_after_pos_tp_rank_0.pt\"\n",
    "REF_Q_AFTER_POS = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states_with_bs_2/acts/query_states_after_pos_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_q_after_pos = torch.load(INFINI_Q_AFTER_POS)\n",
    "ref_q_after_pos = torch.load(REF_Q_AFTER_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 4, 128]), torch.Size([512, 4, 128]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_q_after_pos.shape, ref_q_after_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1719,  0.1797,  1.0469, -0.9727],\n",
       "         [ 0.9023, -0.0422, -0.2871, -0.1855],\n",
       "         [-1.3672, -1.0312,  1.5859, -0.0500],\n",
       "         [-0.3594, -0.8594, -1.9844, -1.1875]],\n",
       "\n",
       "        [[-0.5703, -2.5469,  0.4590,  0.6172],\n",
       "         [ 0.8164,  0.8906, -0.3906, -2.1250],\n",
       "         [ 0.1514, -1.6484,  0.2988,  0.2070],\n",
       "         [-0.9492,  0.6250, -0.8242, -0.9023]],\n",
       "\n",
       "        [[ 0.6133, -0.8125,  1.1094, -1.0625],\n",
       "         [-0.5547, -0.1572, -1.9922, -0.7266],\n",
       "         [ 0.2891, -1.0234, -1.4062, -1.1875],\n",
       "         [ 0.8867,  1.5625,  0.2119, -1.5000]],\n",
       "\n",
       "        [[-2.0312, -0.4199,  0.0208,  0.6680],\n",
       "         [ 0.0962,  0.2910, -0.8867, -0.6016],\n",
       "         [ 0.0991, -0.4004,  0.8477, -0.4434],\n",
       "         [-1.1562,  1.0938,  1.9219, -2.6406]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_q_after_pos[:4, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1719,  0.1797,  1.0469, -0.9727],\n",
       "         [ 0.9023, -0.0422, -0.2871, -0.1855],\n",
       "         [-1.3672, -1.0312,  1.5859, -0.0500],\n",
       "         [-0.3594, -0.8594, -1.9844, -1.1875]],\n",
       "\n",
       "        [[ 1.3594, -0.5586,  0.3867, -0.3340],\n",
       "         [-0.0435, -0.2393, -1.0078,  0.6758],\n",
       "         [ 0.1060, -1.3516, -0.0374, -0.5859],\n",
       "         [ 0.5703,  0.9883,  0.5430, -1.0781]],\n",
       "\n",
       "        [[-0.1748, -2.0000,  0.2852, -0.0811],\n",
       "         [ 0.4785,  0.8359, -0.7344, -1.1641],\n",
       "         [ 0.6523, -1.2344,  1.3984,  1.4297],\n",
       "         [-0.5078,  0.5000,  0.1543, -1.9609]],\n",
       "\n",
       "        [[ 0.5117,  0.6289, -0.9453, -0.2773],\n",
       "         [-1.1094,  0.8984, -1.1953, -0.5078],\n",
       "         [ 0.4531, -0.7031,  0.9336, -1.9062],\n",
       "         [-0.2559, -0.5156,  0.7266,  0.1602]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_q_after_pos[:4, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(infini_q_after_pos, ref_q_after_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output projection in attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_BLOCK_0_ATTN_O_PROJ = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.attn.o_proj_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_BLOCK_0_ATTN_O_PROJ = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.attn.o_proj_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_block_0_attn_o_proj = torch.load(INFINI_BLOCK_0_ATTN_O_PROJ)\n",
    "ref_block_0_attn_o_proj = torch.load(REF_BLOCK_0_ATTN_O_PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 2, 1024]), torch.Size([256, 2, 1024]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_block_0_attn_o_proj.shape, ref_block_0_attn_o_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3438, -0.9492,  0.2832,  0.4766],\n",
       "         [ 0.0796,  0.0781,  0.3281,  0.0620]],\n",
       "\n",
       "        [[-0.3379,  0.3047,  0.5430, -0.2949],\n",
       "         [ 0.2354, -0.2988,  0.6602, -0.3340]],\n",
       "\n",
       "        [[ 0.4922, -0.1660,  0.0825, -0.6328],\n",
       "         [ 0.1621, -0.3633,  0.4121,  0.3984]],\n",
       "\n",
       "        [[ 1.0234, -0.4199, -0.2656, -0.4004],\n",
       "         [ 0.4727, -0.5547,  0.5000, -0.6641]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_block_0_attn_o_proj[:4, :4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3438, -0.9492,  0.2832,  0.4766],\n",
       "         [ 0.0796,  0.0781,  0.3281,  0.0620]],\n",
       "\n",
       "        [[ 0.0723,  0.0486,  0.3301,  0.0781],\n",
       "         [ 0.2041, -0.4375,  0.4199, -0.5117]],\n",
       "\n",
       "        [[-0.2715,  0.3301,  0.4766, -0.2715],\n",
       "         [ 0.0933, -0.6016,  0.8164,  0.1963]],\n",
       "\n",
       "        [[ 0.2256, -0.3301,  0.6680, -0.2949],\n",
       "         [-0.0248, -0.7422,  0.1206,  0.2432]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_block_0_attn_o_proj[:4, :4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_BLOCK_0_ATTN = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.attn.attention_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "# INFINI_AFTER_FIX_BLOCK_0_ATTN = \"/fsx/phuc/projects/nanotron/debug/nn_states_after_fix/acts/model.decoder.0.pp_block.attn.attention_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_BLOCK_0_ATTN = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.attn.attention_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_block_0_attn = torch.load(INFINI_BLOCK_0_ATTN)\n",
    "# infini_after_fix_block_0_attn = torch.load(INFINI_AFTER_FIX_BLOCK_0_ATTN)\n",
    "ref_block_0_attn = torch.load(REF_BLOCK_0_ATTN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 4, 128]), torch.Size([512, 4, 128]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_block_0_attn.shape, ref_block_0_attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8594, -1.2344,  1.8359,  1.7500],\n",
       "         [-0.5586,  0.4570,  0.0569, -0.7695],\n",
       "         [-0.5195, -0.5312,  0.8672, -0.1211],\n",
       "         [-1.7891,  0.5234, -0.1914,  1.0781]],\n",
       "\n",
       "        [[ 0.0708, -0.0742,  0.2324,  1.0312],\n",
       "         [-0.4902, -1.1484, -0.6484, -0.6875],\n",
       "         [ 0.5000,  1.3516,  0.2354, -0.0811],\n",
       "         [-0.2314, -2.1250,  1.0547,  0.5664]],\n",
       "\n",
       "        [[ 0.3047,  0.0173,  0.1953, -0.4941],\n",
       "         [-0.0486,  0.1279,  1.5781,  0.3594],\n",
       "         [ 0.0085, -0.1689,  0.5664,  1.0234],\n",
       "         [-0.7383, -0.4258,  1.4688, -0.0294]],\n",
       "\n",
       "        [[ 0.7188, -0.2852, -0.3516, -0.2412],\n",
       "         [-0.1973,  0.5000,  0.6484,  0.0136],\n",
       "         [ 1.1094,  0.2559,  0.8359, -1.3203],\n",
       "         [ 0.0864,  1.1406, -0.6914, -0.0488]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_block_0_attn[:4, :4, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8594, -1.2344,  1.8359,  1.7500],\n",
       "         [-0.5586,  0.4570,  0.0569, -0.7695],\n",
       "         [-0.5195, -0.5312,  0.8672, -0.1211],\n",
       "         [-1.7891,  0.5234, -0.1914,  1.0781]],\n",
       "\n",
       "        [[-1.0234, -0.5547,  0.9883, -1.3516],\n",
       "         [ 0.2002, -0.0874,  1.7266, -0.8320],\n",
       "         [-0.9180,  1.1484,  0.7148, -1.0703],\n",
       "         [ 0.3457,  1.3281,  0.6562, -1.2344]],\n",
       "\n",
       "        [[-0.0025, -0.1035,  0.2793,  0.8633],\n",
       "         [-0.4668, -1.1406, -0.5820, -0.6914],\n",
       "         [-0.2305,  1.2656,  0.4785, -0.5977],\n",
       "         [-0.1924, -2.0469,  1.0547,  0.4980]],\n",
       "\n",
       "        [[ 0.1748, -0.5938,  0.2090, -0.0942],\n",
       "         [ 0.6680, -0.9531,  0.5781, -0.4355],\n",
       "         [-0.1426,  0.0674, -0.5469, -0.7578],\n",
       "         [-0.8398,  1.1094,  1.0469, -0.4863]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_block_0_attn[:4, :4, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_MLP = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.mlp_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "# INFINI_AFTER_FIX_MLP = \"/fsx/phuc/projects/nanotron/debug/nn_states_after_fix/acts/model.decoder.0.pp_block.mlp_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_MLP = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/model.decoder.0.pp_block.mlp_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_mlp = torch.load(INFINI_MLP)\n",
    "# infini_after_mlp = torch.load(INFINI_AFTER_FIX_MLP)\n",
    "ref_mlp = torch.load(REF_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 1024]), torch.Size([5, 2, 1024]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_mlp[\"hidden_states\"].shape, ref_mlp[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5117, -0.7969,  0.1934,  0.0223],\n",
       "         [-0.5117, -0.7969,  0.1934,  0.0223]],\n",
       "\n",
       "        [[ 0.3066,  0.1270, -0.3691,  0.2656],\n",
       "         [ 0.3066,  0.1270, -0.3691,  0.2656]],\n",
       "\n",
       "        [[ 0.0693, -0.1641,  0.5742,  0.1885],\n",
       "         [ 0.0693, -0.1641,  0.5742,  0.1885]],\n",
       "\n",
       "        [[ 0.6602, -0.2910,  0.2988,  0.5703],\n",
       "         [ 0.6602, -0.2910,  0.2988,  0.5703]],\n",
       "\n",
       "        [[ 0.7188,  0.2539, -0.5859,  0.2734],\n",
       "         [ 0.7188,  0.2539, -0.5859,  0.2734]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_mlp[\"hidden_states\"][:, :, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final layer norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFINI_FINAL_LN = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/model.final_layer_norm_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "# INFINI_AFTER_FIX_FINAL_LN = \"/fsx/phuc/projects/nanotron/debug/nn_states_after_fix/acts/model.final_layer_norm_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_FINAL_LN = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/model.final_layer_norm_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_final_ln = torch.load(INFINI_FINAL_LN)\n",
    "# infini_after_fix_final_ln = torch.load(INFINI_AFTER_FIX_FINAL_LN)\n",
    "ref_final_ln = torch.load(REF_FINAL_LN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 256, 1024]), torch.Size([256, 2, 1024]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_final_ln[\"hidden_states\"].shape, ref_final_ln[\"hidden_states\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3828, -0.3906,  0.6797, -1.1094],\n",
       "         [ 0.7852, -0.9180,  1.0078,  0.0518],\n",
       "         [ 1.1953, -0.2139,  1.3438, -0.4922],\n",
       "         ...,\n",
       "         [ 0.9648, -1.0234,  1.4062, -1.3672],\n",
       "         [ 1.5703, -0.6641,  0.8789, -0.5234],\n",
       "         [ 0.0859, -0.8125,  1.5781, -0.3887]],\n",
       "\n",
       "        [[ 2.1406,  0.0620,  0.5781,  0.6680],\n",
       "         [ 2.0469, -0.2949,  1.2500,  0.6172],\n",
       "         [ 0.5156, -0.4512,  0.9922, -0.3691],\n",
       "         ...,\n",
       "         [ 2.2969, -0.2129,  0.7539, -0.5508],\n",
       "         [ 1.2422, -1.4219,  0.3672,  0.4180],\n",
       "         [ 1.5547, -0.6133,  0.7305, -0.6016]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_final_ln[\"hidden_states\"][:, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3828, -0.3906,  0.6797, -1.1094],\n",
       "         [ 0.5352, -0.4629,  0.3730, -1.0312]],\n",
       "\n",
       "        [[ 0.7656, -0.3809,  0.4316, -1.0391],\n",
       "         [ 1.5547,  0.7852,  0.0095, -0.3027]],\n",
       "\n",
       "        [[ 2.4531,  0.2871,  1.3594,  0.6445],\n",
       "         [ 0.5156, -0.7578,  0.9258, -1.9688]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.3906,  0.1445,  0.3867, -0.7812],\n",
       "         [ 0.4590, -0.6289,  0.7266,  0.4590]],\n",
       "\n",
       "        [[ 0.1885, -0.9336,  0.3164, -0.8477],\n",
       "         [ 0.1367, -0.8008,  0.9219, -0.9961]],\n",
       "\n",
       "        [[ 1.0703, -0.2910,  1.8047,  0.3730],\n",
       "         [ 2.1562, -0.8477,  0.9844, -0.4492]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_final_ln[\"hidden_states\"][:, :, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "INFINI_LOGITS = \"/fsx/phuc/projects/nanotron/debug/nn_states_with_bs_1/acts/model.lm_head_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "# INFINI_AFTER_FIX_LOGITS = \"/fsx/phuc/projects/nanotron/debug/nn_states_after_fix/acts/model.lm_head_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_LOGITS = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states_with_bs_1/acts/model.lm_head_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_logits = torch.load(INFINI_LOGITS)\n",
    "# infini_after_fix_logits = torch.load(INFINI_AFTER_FIX_LOGITS)\n",
    "ref_logits = torch.load(REF_LOGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 256, 24576]), torch.Size([256, 1, 24576]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_logits[\"logits\"].shape, ref_logits[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2188, -2.5156, -4.5312, -3.7969]],\n",
       "\n",
       "        [[ 2.0000, -0.4043, -2.6094, -3.2500]],\n",
       "\n",
       "        [[ 2.7656, -2.9219, -4.9062, -4.1250]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.7266, -1.4375, -4.2188, -4.0312]],\n",
       "\n",
       "        [[ 7.0000, -3.2500, -2.0312, -2.7812]],\n",
       "\n",
       "        [[ 0.9531, -2.3750, -2.9844, -3.6719]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_logits[\"logits\"].transpose(0, 1)[:, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2188, -2.5156, -4.5312, -3.7969]],\n",
       "\n",
       "        [[ 2.0000, -0.4043, -2.6094, -3.2500]],\n",
       "\n",
       "        [[ 2.7656, -2.9219, -4.9062, -4.1250]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.7266, -1.4219, -4.2500, -4.0312]],\n",
       "\n",
       "        [[ 7.0000, -3.2344, -2.0156, -2.7656]],\n",
       "\n",
       "        [[ 0.9570, -2.3594, -2.9844, -3.6562]]], device='cuda:0',\n",
       "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_logits[\"logits\"][:, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(infini_logits[\"logits\"].transpose(0, 1), ref_logits[\"logits\"], rtol=0.09, atol=0.09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharded loss before loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "INFINI_SHARDED_LOGITS_STEP1 = \"/fsx/phuc/projects/nanotron/debug/nn_states/acts/step1_sharded_logits_tp_rank_0.pt\"\n",
    "\n",
    "REF_SHARDED_LOGITS_STEP1 = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states/acts/step1_sharded_logits_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "infini_sharded_logits_step1 = torch.load(INFINI_SHARDED_LOGITS_STEP1)\n",
    "ref_sharded_logits_step1 = torch.load(REF_SHARDED_LOGITS_STEP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 2, 24576]), torch.Size([256, 2, 24576]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_sharded_logits_step1.shape, ref_sharded_logits_step1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2188, -2.5156, -4.5312, -3.7969],\n",
       "         [ 3.0156, -2.3750, -2.5312, -3.5000]],\n",
       "\n",
       "        [[ 3.5938, -3.7500, -4.4062, -2.7500],\n",
       "         [ 7.5938, -3.1094, -3.3281, -5.0312]],\n",
       "\n",
       "        [[ 2.8594, -1.6328, -4.6250, -3.9062],\n",
       "         [ 3.5000, -1.8828, -3.7188, -3.5469]],\n",
       "\n",
       "        [[ 2.2344, -1.6797, -4.5938, -4.4062],\n",
       "         [ 3.3906, -1.3359, -2.1562, -3.9219]]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_sharded_logits_step1[:4, :, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2188, -2.5156, -4.5312, -3.7969],\n",
       "         [ 1.8438, -0.5430, -3.1250, -3.8281]],\n",
       "\n",
       "        [[ 2.0000, -0.4043, -2.6094, -3.2500],\n",
       "         [ 3.2812, -2.0625, -7.0625, -5.2500]],\n",
       "\n",
       "        [[ 2.7656, -2.9219, -4.9062, -4.1250],\n",
       "         [ 2.9062, -3.3438, -3.6406, -4.2188]],\n",
       "\n",
       "        [[ 8.0625, -3.3594, -2.2969, -3.8906],\n",
       "         [ 2.4844, -3.9062, -5.2812, -1.9609]]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_sharded_logits_step1[:4, :, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# INFINI_LOSS = \"/fsx/phuc/projects/nanotron/debug/nn_states_with_bs_1/acts/loss.pp_block_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "INFINI_LOSS_WITH_TRANSPOSED_QKV = \"/fsx/phuc/projects/nanotron/debug/nn_states_with_bs_2_and_transpose_qkv/acts/loss_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "REF_LOSS = \"/fsx/phuc/projects/reference/nanotron/debug/nn_states_with_bs_2/acts/loss_dp_rank_0_and_pp_rank_0_and_tp_rank_0.pt\"\n",
    "\n",
    "\n",
    "# infini_loss = torch.load(INFINI_LOSS)\n",
    "infini_loss_with_transposed_qkv = torch.load(INFINI_LOSS_WITH_TRANSPOSED_QKV)\n",
    "ref_loss = torch.load(REF_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(4.3787, device='cuda:0', requires_grad=True)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infini_loss_with_transposed_qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(4.3784, device='cuda:0', requires_grad=True)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
